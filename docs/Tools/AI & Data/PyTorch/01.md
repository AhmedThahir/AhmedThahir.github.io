## Init

```python
import torch
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader

import torchvision
from torchvision import transforms
from torchvision.datasets import ImageFolder
import timm

import matplotlib.pyplot as plt ## For data viz
import pandas as pd
import numpy as np

import sys
from tqdm.notebook import tqdm

print('System Version:', sys.version)
print('PyTorch version', torch.__version__)
print('Torchvision version', torchvision.__version__)
print('Numpy version', np.__version__)
print('Pandas version', pd.__version__)

device = (
  "mps" if getattr(torch, "has_mps", False)
  else
  "cuda" if torch.cuda().is_available()
  else
  "cpu"
)
```

## Tensors

```python
torch.mean(image_data, axis=0) ## column-wise mean

luminance_approx = torch.mean(image_array, axis=-1) ## color_channel-wise mean

values, indices = torch.max(data, axis=-1)
```

```
- int8 is an integer type, it can be used for any operation which needs integers
- qint8 is a quantized tensor type which represents a compressed floating point tensor, it has an underlying int8 data layer, a scale, a zero_point and a qscheme
```

## API

## Syntax

```python
  model = nn.Linear(in_features=8, out_features=1, bias=False)
  y = model(x).squeeze()
```

## Basic Model

```python
  class Model(nn.Module):
    def __init__(self):
      super().__init__()
      self.matrix_layer1 = nn.Linear(2, 8, bias=False)
      self.matrix_layer2 = nn.Linear(8, 1, bias=False)

    def forward(self, X):
      y_layer1 = self.m1(X)
      y_layer2 = self.m2(y_layer1)
      return y_layer2.squeeze()

  model = Model()
  model = torch.compile(model, backend = "aot_eager").to(device)

  L = nn.MSELoss()

  opt = SGD(model.parameters(), lr=0.001)
  losses = []

  EPOCHS_MAX = 50
  for i in range(EPOCHS_MAX):
    opt.zero_grad() ## clear previous epoch gradient
    loss = L(model(x), y)
    loss.backward() ## compute gradient
    opt.step()
    losses.append(loss_value.item())
```

## Dataloader

```python
  test_loader = DataLoader(
    dataset,
    batch_size=32,
    shuffle=True,
    num_workers=1 ## number of *sub*-processes (other than the main process) that will be used for loading images while data is being trained on the main process; setting this high adds a lot of overhead
  )
```

## Lazy Layers

automatically detect the input size

```python
  class NN(nn.module):
    def __init__(self, num_classes):
      super().__init__()

      self.fully_connected_layer = nn.Sequential(
        nn.LazyLinear(512) ## no need to specify input size
      )
```

## Save Model

```python
  torch.save(model, "model.pkl")
```

## View Parameters

```python
  for param in model.parameters():
    print(name)

  for name, param in model.named_parameters():
      if param.requires_grad:
          print(name, param.data)
```

## Networks

## IDK

```python
```

## Shallow Network

```python
  class Exponential_Model(nn.Module):
      def __init__(self):
          super().__init__()
          self.w  = nn.Parameter(torch.randn(1,dtype=torch.float64), requires_grad=True)
          self.b  = nn.Parameter(torch.randn(1,dtype=torch.float64), requires_grad=True)

      def forward(self,x):
          return torch.exp(self.w *x + self.b)

  model = CustomModel().double()
  criterion = nn.MSELoss()

  optimizer = torch.optim.SGD(model.parameters(), lr = 0.0001)

  y_pred_dict = {}
  weights_dict = {}
  biases_dict  = {}

  model.train()
  MAX_EPOCHS = 100
  for epochs in range(MAX_EPOCHS):

      output = model(X)       ##### forward pass
      loss   = criterion(output,Y) #### loss calculation
      optimizer.zero_grad()        #### gradients to 0
      loss.backward()              #### backward prop
      optimizer.step()             #### Updated the w and b

      if (epochs) % 5 == 0:
          print(f'Epoch {epochs} Loss : {loss.item()}')
          y_pred_dict[epochs] = output
          weights_dict[epochs] = model.w.item()
          biases_dict[epochs]  = model.b.item()


  y_pred_dict[epochs] = output
  weights_dict[epochs] = model.w.item()
  biases_dict[epochs]  = model.b.item()
```

## Regression

```python
  a = torch.ones(5, 10 )

  sums = torch.sum(a, axis=0) ## sum of each column
  sums = torch.sum(a, axis=1) ## sum of each row
```

Define model using `nn.module`

Cost function

Optimizer

Epochs

Train data

Predict

```python
  class Prediction(nn.Module):
    def __init__(self, k, output_neurons):
      super().__init__()

      self.linear_layer = nn.Linear(k, output_neurons)
      self.sigmoid_layer = nn.Sigmoid()
    def forward(self, X):
      two_neurons_output = self.linear_layer(X) ## n x 2
      pre_sigmoid_input = torch.mean(two_neurons_output, axis=1) ## n x 1

      activated_output = self.sigmoid_layer(pre_sigmoid_input)

      return activated_output

  model = Prediction(3, 2)
  loss_function = nn.CrossEntropyLoss()
  optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

  model.train()
  num_iterations = 1_000
  for i in range(num_iterations):
    pred = model(X_train)

    loss = loss_function(pred, true)

    if i % 100 == 0:
      print(loss.item()) 

    loss.backward() ## will calculate all necessary derivatives

    optimizer.zero_grad() ## clear optimizer
    optimizer.step() ## use derivatives to update weights -> weight_new = weight_prev - (der*learning_rate)


  model.eval()
```

## Classifier

## PyTorch Data

We can then wrap the dataset in a dataloader and pytorch will handle batching the shuffling the data for us when training the model!

## Dataset

```python
      class PlayingCardDataset(Dataset):
          def __init__(self, data_dir, transform=None):
              self.data = ImageFolder(data_dir, transform=transform) ## automatically assumes that foldername is class label

          def __len__(self):
              return len(self.data)

          def __getitem__(self, idx):
              return self.data[idx]

          @property
          def classes(self):
              return self.data.classes ## classes of images (not python classes)
```

```python
      transform = transforms.Compose([
          transforms.Resize((128, 128)),
          transforms.ToTensor(),
      ])

      train_folder = '/kaggle/input/cards-image-datasetclassification/train'
      valid_folder = '../input/cards-image-datasetclassification/valid/'
      test_folder = '../input/cards-image-datasetclassification/test/'

      train_dataset = PlayingCardDataset(train_folder, transform=transform)
      val_dataset = PlayingCardDataset(valid_folder, transform=transform)
      test_dataset = PlayingCardDataset(test_folder, transform=transform)
```

```python
      for image, label in train_dataset:
        print(image.shape) ## (channels, width, height)
          print(label.shape) ## (1)
          break
```

```python
      ## Get a dictionary associating target values with folder names
      target_to_class = {v: k for k, v in ImageFolder(data_dir).class_to_idx.items()}
      print(target_to_class)
```

## Dataloader

Why?

Batching our dataset

It's faster to train the model in batches instead of one at a time.

```python
      train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=1)
      val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=1)
      test_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=1)
```

```python
      for images, labels in train_loader:
          print(images.shape) ## (batch_size, channels, width, height)
          print(labels.shape) ## (batch_size)
          break
```

## PyTorch Model

```python
    class SimpleCardClassifer(nn.Module):
        def __init__(self, num_classes=53):
            super().__init__()

            ## Where we define all the parts of the model
            self.base_model = timm.create_model('efficientnet_b0', pretrained=True)

            ## remove the last layer
            self.features = nn.Sequential(*list(self.base_model.children())[:-1])
            enet_out_size = 1280

            ## add a classifier for our own requirement
            self.classifier = nn.Sequential(
                nn.Flatten(),
                nn.Linear(enet_out_size, num_classes)
            )

        def forward(self, x):
            ## Connect these parts and return the output
            x = self.features(x)
            output = self.classifier(x)
            return output
```

```python
    model = SimpleCardClassifer(num_classes=53)
```

```python
    print(str(model)[:500])

    example_out = model(images)
    example_out.shape ## [batch_size, num_classes]
```

## PyTorch Training Loop

Terms

Epoch: One run through the entire training dataset.

Step: One batch of data as defined in our dataloader

Two things to select:

optimizer: `adam` is the best place to start for most tasks.

loss function: What the model will optimize for.

```python
    ## Loss function
    criterion = nn.CrossEntropyLoss()
    ## Optimizer
    optimizer = optim.Adam(model.parameters(), lr=0.001)
```

```python
    criterion(example_out, labels)
    print(example_out.shape, labels.shape)
```

```python
     ## Simple training loop
    num_epochs = 5
    train_losses, val_losses = [], []

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    model = SimpleCardClassifer(num_classes=53)
    model.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    for epoch in range(num_epochs):
        ## Training phase
        model.train()
        running_loss = 0.0
        for images, labels in tqdm(train_loader, desc='Training loop'):
            ## Move inputs and labels to the device
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * labels.size(0)
        train_loss = running_loss / len(train_loader.dataset)
        train_losses.append(train_loss)

        ## Validation phase
        model.eval()
        running_loss = 0.0

        with torch.no_grad():
            for images, labels in tqdm(val_loader, desc='Validation loop'):
                ## Move inputs and labels to the device
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                running_loss += loss.item() * labels.size(0)

        val_loss = running_loss / len(val_loader.dataset)
        val_losses.append(val_loss)
        print(f"Epoch {epoch+1}/{num_epochs} - Train loss: {train_loss}, Validation loss: {val_loss}")

    ## Testing phase
    model.eval()
```

```python
    plt.plot(train_losses, label='Training loss')
    plt.plot(val_losses, label='Validation loss')
    plt.legend()
    plt.title("Loss over epochs")
    plt.show() 
```

## Hacks?

## Custom Loss Function

```python
  class loss(nn.module):
    def forward(self, pred, y):
      error = pred-y
      return torch.mean(
        torch.abs(error)
      )
```

## Uncertainty

```python
  import numpy as np
  import torch, torchvision
  from torch.autograd import Variable, grad
  import torch.distributions as td
  import math
  from torch.optim import Adam
  import scipy.stats


  x_data = torch.randn(100)+0.0 ## observed data (here sampled under H0)

  N = x_data.shape[0] ## number of observations

  mu_null = torch.zeros(1)
  sigma_null_hat = Variable(torch.ones(1), requires_grad=True)

  def log_lik(mu, sigma):
    return td.Normal(loc=mu, scale=sigma).log_prob(x_data).sum()

  ## Find theta_null_hat by some gradient descent algorithm (in this case an closed-form expression would be trivial to obtain (see below)):
  opt = Adam([sigma_null_hat], lr=0.01)
  for epoch in range(2000):
      opt.zero_grad() ## reset gradient accumulator or optimizer
      loss = - log_lik(mu_null, sigma_null_hat) ## compute log likelihood with current value of sigma_null_hat  (= Forward pass)
      loss.backward() ## compute gradients (= Backward pass)
      opt.step()      ## update sigma_null_hat

  print(f'parameter fitted under null: sigma: {sigma_null_hat}, expected: {torch.sqrt((x_data**2).mean())}')
  #> parameter fitted under null: sigma: tensor([0.9260], requires_grad=True), expected: 0.9259940385818481

  theta_null_hat = (mu_null, sigma_null_hat)

  U = torch.tensor(torch.autograd.functional.jacobian(log_lik, theta_null_hat)) ## Jacobian (= vector of partial derivatives of log likelihood w.r.t. the parameters (of the full/alternative model)) = score
  I = -torch.tensor(torch.autograd.functional.hessian(log_lik, theta_null_hat)) / N ## estimate of the Fisher information matrix
  S = torch.t(U) @ torch.inverse(I) @ U / N ## test statistic, often named "LM" (as in Lagrange multiplier), would be zero at the maximum likelihood estimate

  pval_score_test = 1 - scipy.stats.chi2(df = 1).cdf(S) ## S asymptocially follows a chi^2 distribution with degrees of freedom equal to the number of parameters fixed under H0
  print(f'p-value Chi^2-based score test: {pval_score_test}')
  #> p-value Chi^2-based score test: 0.9203232752568568

  ## comparison with Student's t-test:
  pval_t_test = scipy.stats.ttest_1samp(x_data, popmean = 0).pvalue
  print(f'p-value Student\'s t-test: {pval_t_test}')
  #> p-value Student's t-test: 0.9209265268946605
```

```python
  ## another example

  env_loss = loss_fn(env_outputs, env_targets)
  total_loss += env_loss
  env_grads = torch.autograd.grad(env_loss, params, retain_graph=True, create_graph=True)

  print(env_grads[0])
  hess_params = torch.zeros_like(env_grads[0])
  for i in range(env_grads[0].size(0)):
      for j in range(env_grads[0].size(1)):
          hess_params[i, j] = torch.autograd.grad(env_grads[0][i][j], params, retain_graph=True)[0][i, j] ##  <--- error here
  print(hess_params)
```

## Early-Stopping

Class

```python
    import copy


    class EarlyStopping:
        def __init__(self, patience=5, min_delta=0, restore_best_weights=True):
            self.patience = patience
            self.min_delta = min_delta
            self.restore_best_weights = restore_best_weights
            self.best_model = None
            self.best_loss = None
            self.counter = 0
            self.status = ""

        def __call__(self, model, val_loss):
            if self.best_loss is None:
                self.best_loss = val_loss
                self.best_model = copy.deepcopy(model.state_dict())
            elif self.best_loss - val_loss >= self.min_delta:
                self.best_model = copy.deepcopy(model.state_dict())
                self.best_loss = val_loss
                self.counter = 0
                self.status = f"Improvement found, counter reset to {self.counter}"
            else:
                self.counter += 1
                self.status = f"No improvement in the last {self.counter} epochs"
                if self.counter >= self.patience:
                    self.status = f"Early stopping triggered after {self.counter} epochs."
                    if self.restore_best_weights:
                        model.load_state_dict(self.best_model)
                    return True
            return False
```

Classification

```python
    import time

    import numpy as np
    import pandas as pd
    import torch
    import tqdm
    from sklearn.metrics import accuracy_score
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import LabelEncoder, StandardScaler
    from torch import nn
    from torch.autograd import Variable
    from torch.utils.data import DataLoader, TensorDataset

    ## Set random seed for reproducibility
    np.random.seed(42)
    torch.manual_seed(42)

    def load_data():
        df = pd.read_csv(
            "https://data.heatonresearch.com/data/t81-558/iris.csv", na_values=["NA", "?"]
        )

        le = LabelEncoder()

        x = df[["sepal_l", "sepal_w", "petal_l", "petal_w"]].values
        y = le.fit_transform(df["species"])
        species = le.classes_

        ## Split into validation and training sets
        x_train, x_test, y_train, y_test = train_test_split(
            x, y, test_size=0.25, random_state=42
        )

        scaler = StandardScaler()
        x_train = scaler.fit_transform(x_train)
        x_test = scaler.transform(x_test)

        ## Numpy to Torch Tensor
        x_train = torch.tensor(x_train, device=device, dtype=torch.float32)
        y_train = torch.tensor(y_train, device=device, dtype=torch.long)

        x_test = torch.tensor(x_test, device=device, dtype=torch.float32)
        y_test = torch.tensor(y_test, device=device, dtype=torch.long)

        return x_train, x_test, y_train, y_test, species


    x_train, x_test, y_train, y_test, species = load_data()

    ## Create datasets
    BATCH_SIZE = 16

    dataset_train = TensorDataset(x_train, y_train)
    dataloader_train = DataLoader(
        dataset_train, batch_size=BATCH_SIZE, shuffle=True)

    dataset_test = TensorDataset(x_test, y_test)
    dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=True)

    ## Create model using nn.Sequential
    model = nn.Sequential(
        nn.Linear(x_train.shape[1], 50),
        nn.ReLU(),
        nn.Linear(50, 25),
        nn.ReLU(),
        nn.Linear(25, len(species)),
        nn.LogSoftmax(dim=1),
    )

    model = torch.compile(model,backend="aot_eager").to(device)

    loss_fn = nn.CrossEntropyLoss()  ## cross entropy loss

    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    es = EarlyStopping()

    epoch = 0
    done = False
    while epoch < 1000 and not done:
        epoch += 1
        steps = list(enumerate(dataloader_train))
        pbar = tqdm.tqdm(steps)
        model.train()
        for i, (x_batch, y_batch) in pbar:
            y_batch_pred = model(x_batch.to(device))
            loss = loss_fn(y_batch_pred, y_batch.to(device))
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            loss, current = loss.item(), (i + 1) * len(x_batch)
            if i == len(steps) - 1:
                model.eval()
                pred = model(x_test)
                vloss = loss_fn(pred, y_test)
                if es(model, vloss):
                    done = True
                pbar.set_description(
                    f"Epoch: {epoch}, tloss: {loss}, vloss: {vloss:>7f}, {es.status}"
                )
            else:
                pbar.set_description(f"Epoch: {epoch}, tloss {loss:}")
```

Regression

```python
    import time

    import numpy as np
    import pandas as pd
    import torch.nn as nn
    import torch.nn.functional as F
    import tqdm
    from sklearn import preprocessing
    from sklearn.metrics import accuracy_score
    from sklearn.model_selection import train_test_split
    from torch.autograd import Variable
    from torch.utils.data import DataLoader, TensorDataset

    ## Read the MPG dataset.
    df = pd.read_csv(
        "https://data.heatonresearch.com/data/t81-558/auto-mpg.csv", na_values=["NA", "?"]
    )

    cars = df["name"]

    ## Handle missing value
    df["horsepower"] = df["horsepower"].fillna(df["horsepower"].median())

    ## Pandas to Numpy
    x = df[
        [
            "cylinders",
            "displacement",
            "horsepower",
            "weight",
            "acceleration",
            "year",
            "origin",
        ]
    ].values
    y = df["mpg"].values  ## regression

    ## Split into validation and training sets
    x_train, x_test, y_train, y_test = train_test_split(
        x, y, test_size=0.25, random_state=42
    )

    ## Numpy to Torch Tensor
    x_train = torch.tensor(x_train, device=device, dtype=torch.float32)
    y_train = torch.tensor(y_train, device=device, dtype=torch.float32)

    x_test = torch.tensor(x_test, device=device, dtype=torch.float32)
    y_test = torch.tensor(y_test, device=device, dtype=torch.float32)


    ## Create datasets
    BATCH_SIZE = 16

    dataset_train = TensorDataset(x_train, y_train)
    dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)

    dataset_test = TensorDataset(x_test, y_test)
    dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=True)


    ## Create model

    model = nn.Sequential(
        nn.Linear(x_train.shape[1], 50), 
        nn.ReLU(), 
        nn.Linear(50, 25), 
        nn.ReLU(), 
        nn.Linear(25, 1)
    )

    model = torch.compile(model, backend="aot_eager").to(device)

    ## Define the loss function for regression
    loss_fn = nn.MSELoss()

    ## Define the optimizer
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

    es = EarlyStopping()

    epoch = 0
    done = False
    while epoch < 1000 and not done:
        epoch += 1
        steps = list(enumerate(dataloader_train))
        pbar = tqdm.tqdm(steps)
        model.train()
        for i, (x_batch, y_batch) in pbar:
            y_batch_pred = model(x_batch).flatten()  #
            loss = loss_fn(y_batch_pred, y_batch)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            loss, current = loss.item(), (i + 1) * len(x_batch)
            if i == len(steps) - 1:
                model.eval()
                pred = model(x_test).flatten()
                vloss = loss_fn(pred, y_test)
                if es(model, vloss):
                    done = True
                pbar.set_description(
                    f"Epoch: {epoch}, tloss: {loss}, vloss: {vloss:>7f}, EStop:[{es.status}]"
                )
            else:
                pbar.set_description(f"Epoch: {epoch}, tloss {loss:}")
```

## Dropout

```python
  ## p = p_drop; NOT p_keep like Tensorflow
  model = nn.Sequential(
    nn.Dropout(p=0.2),
    ## ...,
    nn.Dropout(p=0.2),
    ## ...,
  )

  ## make sure to specify model.train() and model.eval(), as dropout processing is only required for "building" the network. no need to processing for evaluation
```

## Quantization

```python
  ## direct precision reduction
  model = model.half() ## changes everything from float32 to float16

  ## dynamic precision reduction
  model = torch.quantization.quantize_dynamic(
    model,
    {torch.nn.Linear},
    dtype=torch.qint8
  )

  ## static precision reduction
  ## very complicated ngl
```
